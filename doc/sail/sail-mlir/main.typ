#import "@preview/charged-ieee:0.1.3": ieee
#import "@preview/algo:0.3.6": algo, code, comment, d, i
#import "@preview/pintorita:0.1.4"
#import "@preview/cetz:0.3.4": canvas, draw, palette
#import "@preview/cetz-plot:0.1.1": chart

#show raw.where(lang: "pintora"): it => pintorita.render(
  it.text,
  factor: none,
  style: "neutralGray",
)

#show: ieee.with(
  title: [An MLIR-based acceleration method for ISA simulator generated by Sail],
  abstract: [
    Sail is a domain-specific language (DSL) for instruction-set architecture, enabling the automatic generation of simulators.
    While this method of using a DSL to generate simulators and related tools is increasingly adopted, such automatically generated simulators are generally considered less efficient than industrial simulators, significantly limiting the specific applications like high-performance simulation.
    This paper innovatively presented a MLIR-based compilation backend for Sail, which is the first approach that integrates MLIR toolchain specifically for Sail IR optimization.
    Different from existing schemes, this backend also offering good scalability and space for further optimization with MLIR's multi-level abstract representation capabilities and flexible optimization infrastructure.
    In particular, this paper created a brand new dialect for Sail language for appling a series of compiler optimizations, including an efficient specialized transformation for Sail arbitrary precision bit-vector arithmetic and deeply customized optimization for key hot functions like step and decode, which were integrated with LLVM JIT.
    The MLIR based optimization significantly boosted the generated simulator. Besides, the MLIR-based dialect opens up a lot of possibilities for further development.
  ],
  authors: (
    (
      name: "Mingzhu Yan",
      department: [Engineer],
      organization: [PLCT Lab],
      location: [Beijing, China],
      email: "yanmingzhu@iscas.ac.cn",
    ),
    (
      name: "Shuo Huang",
      department: [Engineer],
      organization: [PLCT Lab],
      email: "huangshuo4@gmail.com",
    ),
    (
      name: "Yunxiang Luo",
      department: [Test Director],
      organization: [PLCT Lab],
      location: [Beijing, China],
      email: "luoyunxiang@iscas.ac.cn",
    ),
  ),
  index-terms: ("RISC-V", "Formal Verification", "MLIR", "Sail", "just-in-time compilation"),
  bibliography: bibliography("refs.bib"),
  figure-supplement: [Fig.],
)

#show figure.caption: set align(center)
#show figure: set figure(kind: "Fig", supplement: [Fig])

= Introduction

Modern Instruction Set Architecture (ISA)@isa is becoming increasingly complex with growing demands, which raises new requirements for formal specification, verification, and simulation of ISAs. Domain-Specific Language like Sail@sail is a powerful tool for achieving this goal, allowing engineers to precisely describe the semantics of an ISA. Sail can automatically generate various artifacts from the Sail DSL, including documentation, tests, SMT, and executable simulator. Notably, Sail-riscv@sail-riscv, a formal specification of the RISC-V architecture written in Sail, has been adopted by the RISC-V Foundation.

While automatically generating simulators from formal specifications offers significant advantages in terms of correctness and consistency, these generated simulators often face a considerable performance gap compared to highly optimized, manually coded simulators used in industrial environments. Industrial simulators typically incorporate extensive manual optimizations, using dynamic binary translation and hardware acceleration to achieve the exceptional performance required for tasks like booting operating systems or running complex software stacks. Generated simulators, on the other hand, often rely on more general compilation strategies and libraries, which can lead to performance bottlenecks and make them unsuitable for performance-critical simulation tasks. Bridging this gap requires advanced compiler optimization techniques specifically tailored to the simulation workloads.

This paper addresses this challenge by introducing a new Sail compilation backend based on the MLIR@mlir framework. The main contributions include:
The first effort to utilize the MLIR toolchain for optimizing Sail.
The design and implementation of Jib dialect.
The development and application of specialized MLIR-based compiler optimization techniques to address known performance issues in Sail-generated code.
Integration with the LLVM JIT framework to enable Just-In-Time compilation and execution of the optimized simulator code.
By leveraging MLIR's flexible and extensible infrastructure, this paper provides a scalable path for developing domain-specific optimizations for Sail, aiming to significantly accelerate generated ISA simulators and narrow the performance gap with industrial-grade solutions.

= Methods <sec:methods>

Sail's standard compilation flow includes translating the Sail source code into an internal Abstract Syntax Tree, applying various passes, type checking, then chose different backends, such as C, OCaml, or SMT solvers as needed for further processing. The native C backend, which is written in OCaml, first translates the AST into an intermediate language called Jib and finally generates C language source code.

#figure(
  rect[
    ```pintora
    activityDiagram
      :Sail Source Code;
      -> Sail frontend;
      :AST;
      -> Sail backend;
      :Jib;
      :C Source Code;
    ```],
  caption: [Sail compile progress],
) <fig:sail_compile_process>

== Jib Dialect

We chose to write a dialect starting for the Jib IR. Although Jib has a lower level of abstraction than the original Sail source code, it retains some relatively high-level programming constructs. These include various primitive data types, complex data structures such as struct and vector, functions, basic arithmetic and logical operations, and all the arbitrary-precision bit-vector operations, as well as error handling mechanisms. A significant difference between Jib and structured languages is the way its control flow is represented. Jib does not retain traditional structured control flow constructs such as if-then-else and loops. All control flow are simplified to be represented using goto statements. Take ITYPE as an example:

```

X(rd) = match op {
    ADDI  => X(rs1) + immext,
    ...
}


```

This is the original Sail code, which uses an enumeration variable op to represent the current instruction and selects different branches through pattern match. Then in the compiled Jib code, `zz4102` is the enumeration variable, and jump cmd uses `\@neq` to compare the enumeration types, deciding whether to jump to the location of the next comparison.

#figure(
  rect[#code(stroke: 0pt, fill: none)[
      ```
      zz4102 : %enum ziop;
      zz4102 = zmergez3var
          as zITYPE.ztuple_ziop3;
      jump @neq(zRISCV_ADDI, zz4102)
          goto 275;  // slti
      ...               // execute addi
      goto 344;      // end
      jump @neq(zRISCV_SLTI, zz4102)
          goto 294;  // next op
      ```
    ]],
  caption: [Jib jump/goto],
  kind: "Fig.",
) <fig:jib_expand>

To represent Jib in MLIR, we designed the Jib dialect. This dialect defines MLIR operations (mlir::Operation) and types (mlir::Type) that correspond directly to Jib constructs. For example, Jib's complex data types like structs and vectors are mapped to corresponding LLVM IR type. Jib's operations, including bit-vector operations, are represented as custom operations within the Jib dialect. This allows us to preserve the semantic information of Jib operations before applying lowering and optimization passes. This approach of creating a dedicated dialect provides a clear separation of concerns and offers a well-defined intermediate representation at an abstraction level suitable for Sail-specific optimizations.

While most Jib constructs have natural counterparts in existing MLIR dialects (e.g., arithmetic operations map to the arith dialect, function calls map to the llvm.func), goto statements are slightly different. Unlike goto statements that typically target labels, Jib's goto targets relative line numbers within the current function. To transform this into MLIR's Static Single Assignment (SSA)-based Control Flow Graph (CFG) model, we process the Jib code function by function, within each Jib function, operations are split into many basic blocks, goto statements are subsequently translated into `cf.br` operations targeting the corresponding MLIR basic blocks. the relative line number targets of Jib's goto are mapped to specific basic blocks. This transformation effectively converts Jib's linear code with explicit relative jumps into MLIR's structured CFG.

== Optimize gmp operations

The bit-vector operations in Sail is depends on gmp library@gmp, one of the most issue is the enormous size of the generated C code. The original Sail model code is only about 1 MB in total, but the generated C code can be up to around 10 MB, an overall tenfold increase. Taking the following code as an example, ITYPE-related instructions require sign_extend of immediate values before participating in operations:

```

let immext : xlenbits = sign_extend(imm);
X(rd) =  X(rs1) + immext,


```

However the translated C code, specifically the code related to complex computations, inflates by up to four times. Besides the sign extension itself, this includes the addition of multiple lines of code related to type conversion, such as the conversion from `mach_int` to `sail_int`.

#figure(
  rect[#code(stroke: 0pt, fill: none)[
      ```
      uint64_t zimmext;
      {
        sail_int z3zE29675;
        CONVERT_OF(sail_int, mach_int)(&z3zE29675, INT64_C(64));
        lbits z3zE29676;
        CONVERT_OF(lbits, fbits)(&z3zE29676, zuz33826, UINT64_C(12) , true);
        lbits z3zE29677;
        zsign_extend(&z3zE29677, z3zE29675, z3zE29676);
        zimmext = CONVERT_OF(fbits, lbits)(z3zE29677, true);
      }
      ```
    ]],
  caption: [Generated C source code of ADDI],
) <fig:bv-expansion>

In addition to the code size, we also conducted a performance analysis of the compiled simulator with perf@perf. By using the perf tool, we statistically measured the proportion of CPU time spent in different functions under real workloads. The results after merging are as follows

#let data2 = (
  ([gmp], 55.39),
  ([int], 14.21),
  ([pmp], 12.62),
  ([bit], 5.07),
  ([str], 4.72),
)

#let my_value_style(value) = {
  // 假设这个函数能获取到值
  // 假设根据 value 计算一个亮度 L (0-100)，value越大L越大
  let lightness = value // 伪函数
  // 返回一个颜色，亮度由 lightness 决定
  return palette.orange(lightness)
}

#figure(
  rect[#canvas({
      draw.set-style(legend: (fill: white), barchart: (bar-width: .8, cluster-gap: 0))
      chart.barchart(
        data2,
        size: (7, auto),
        x-tick-step: 10,
        x-label: "CPU Time Percent",
        y-label: "Operation Category",
        bar-style: my_value_style,
      )
    })],
  caption: [CPU Time Percentage of Different Operations],
) <tab:fire_percent>

From the table, it can be seen that GMP operations occupy over half of the CPU time. Here, GMP includes all calls to the GMP library API. Following closely are integer operations, PMP checks, general bitwise operations, and string operations. In the context of fixed-width ISAs like RV32 and RV64, the vast majority of bit-vector operations are performed on values that fit within standard machine integer types (e.g., 32-bit or 64-bit integers). Arbitrary precision is strictly necessary only when dealing with very large intermediate values or specific architectural features that exceed the native word length, which is rare in typical instruction semantics. Relying on GMP for all bit-vector operations, even those involving small, fixed-width values, introduces unnecessary overhead.

To address this issue, we designed and implemented a specialized transformation pass in the MLIR backend targeting Jib dialect operations that represent bit-vector arithmetic. This transformation analyzes bit-vector operations to determine if their operands and results have a statically known fixed width that fits within standard integer types (e.g., i32, i64). If the width is within the limits of native types, the Jib bit-vector operations are lowered to a series of operations using MLIR's standard arith dialect, which operate on fixed-width integer types (mlir::IntegerType). For example, a Jib arbitrary-precision bitwise AND operation on two 64-bit values would be lowered to an arith.andi operation on i64 types. Operations like zero extension and sign extension are also translated into corresponding arith or bit operations.

== Optimization decode function

ISA simulators typically follow a fetch-decode-execute cycle. Performance profiling studies of simulators often show that the instruction decoding phase is a significant performance bottleneck, accounting for a substantial portion of the total simulation time. This is especially true for complex ISAs with variable-length instructions or intricate decoding logic.

In Sail, instruction decoding is typically defined using mapping types and the clause keyword. A mapping defines a bidirectional relationship, while clause allows for defining multiple mapping entries with the same name but different input patterns. For instruction decoding, users generally define a mapping named decode, where different clause entries match various instruction bit patterns and map them to an Abstract Syntax Tree.

```

mapping clause encdec =
  ITYPE(imm, rs1, rd, op)
  <-> imm @ encdec_reg(rs1)
    @ encdec_iop(op) @ encdec_reg(rd)
    @ 0b0010011


```

When the standard Sail compiler translates such mapping definitions with clause into Jib (and subsequently C code), it typically generates a series of checks and goto statements. Each clause might be translated into a block of Jib code that first checks if the input matches the pattern of that clause. If it matches, the body of the clause is executed; otherwise, a goto is used to jump to the code block for the next clause. This results in a long sequence of cascaded conditional jumps instead of a jump table which can achieve O(1) computational complexity in many cases. While the standard compiler might apply some level of branch prediction or optimization to such sequences, they are fundamentally less efficient compared to a direct jump table, especially when there are a large number of instruction variants.
In the MLIR backend, after the Jib code is translated into the Jib dialect, a specialized optimization pass is first applied to it to identify the MLIR patterns corresponding to the cascaded decode checks originating from Sail mappings. This pass analyzes the control flow graph structure (sequences of conditional and unconditional branches/gotos) and the operations performing pattern matching checks. If the pattern can be simplified to checking specific bit ranges and values, a multi-way branch is generated using `cf.switch`, potentially allowing the backend compiler (LLVM) to generate an efficient jump table@branch-table.

#figure(
  rect[
    ```pintora
    dotDiagram
      %% pintora style comment
      %% here we declare a directed graph
      digraph G {
        // specify graph attributes
        bgcolor="white"

        // specify common node attributes
        node [color="#111",bgcolor=orange]

        subgraph S1 {
          // subgraph will inherit parent attributes
          label="Goto";
          start1;
          end1;
          ADDI;
          ...;
          XORI;
          start1 -> ADDI;
          start1 -> end1;
          ADDI -> ...;
          ADDI -> end1;
          ... -> XORI;
          ... -> end1;
          XORI -> end1;
        }

        subgraph S2 {
          // subgraph will inherit parent attributes
          label="Switch";

          start2 [fontcolor="purple"];
          end2 [fontcolor="purple"];
          ADDI2 [fontcolor="purple"];
          ...2 [fontcolor="purple"];
          XORI2 [fontcolor="purple"];
          start2 -> ADDI2;
          start2 -> ...2;
          start2 -> XORI2;
          ADDI2 -> end2;
          ...2 -> end2;
          XORI2 -> end2;
        }
      }

    ```],
  caption: [Data flow of different operations],
)

== integrate with JIT

Different simulators use different optimization methods. For example, QEMU can perform full system simulation through its efficient dynamic binary translation@dtb. Some newer user-level simulators also have unique techniques, such as Box64 which converts calls to target library functions into calls to native host libraries when simulating software. There's also Apple's Rosetta2, which performs full binary translation when a program is first run to achieve better execution speed. However, Sail, as a completely new DSL specifically serving ISAs, must possess full system simulation capabilities. Moreover, Sail describes the behavior of instructions based on automatically generated code for the simulator implementation, making it difficult to implement instruction set translation on top of it. The operational process of a Sail simulator is more like a traditional interpreter, where the ELF program being interpreted, and its assembly instructions, are like bytecode. Therefore, to enable dynamic compilation and execution of the optimized Sail code, we integrated the MLIR compilation pipeline with the LLVM Just-In-Time (JIT@jit) framework, which has been proven that it may have better performance in some scenarios@clangjit. JIT compilation is particularly well-suited for ISA simulators because the sequence of instructions to be simulated is not known ahead of time (AOT). The simulator processes a dynamic stream of target instructions. The JIT compiles the LLVM IR of specific functions (e.g., the main step function, the decode function, and potentially Jib functions corresponding to the semantics of individual instructions) into host machine code at runtime. This compiled code is then executed directly. This allows the simulator to benefit from LLVM's highly optimized code generation capabilities tailored to the host architecture.



= Discussion <sec:discussion>

By leveraging MLIR, a modern, extensible compiler infrastructure, for targeted optimizations, the performance of Sail-generated simulators can be significantly improved. The observed substantial speedup is primarily attributed to the specialized transformations we applied:

Bit-vector optimization: By identifying and lowering fixed-width bit-vector operations from opaque GMP representations to native integer operations within the arith dialect, we eliminated the overhead of library calls and enabled standard, highly-optimized compilers to generate code for these frequent operations. This is crucial for ISAs where bit manipulation is prevalent.

Decoding optimization: Converting the cascaded conditional checks generated from Sail's mappings into potentially more efficient control flow structures (like cf.switch) allows the LLVM backend to generate better code for the critical decoding path, reducing the time spent on instruction dispatch.

MLIR and JIT: The use of MLIR provided the necessary infrastructure to systematically define and apply these domain-specific transformations. Integration with the LLVM JIT ensures that the optimized IR is compiled into high-performance native code at runtime, avoiding the limitations of AOT compilation for dynamic workloads and supporting potential future optimizations like profile-guided feedback.

Our work contributes to making DSLs used for hardware description and verification more practical in performance-sensitive applications. While DSL languages and their related tools and ecosystems in the field of processor architecture have developed for decades, the practicality of such tools still remains low. By employing MLIR as a common intermediate representation framework, we offer a promising path towards building more general compiler toolchains for these processor architecture description languages. By defining dialects capable of capturing the semantics of these DSLs, we can leverage MLIR's growing collection of generic optimization passes and analysis tools, while also enabling the implementation of domain-specific transformations such as the bit-vector and decoding optimizations presented in this paper. This approach fosters modularity and reusability of compiler components across different DSLs or different versions of the same ISA.

this work is a foundational step. Several future research directions exist:
Optimize Other Hotspots: Identify and optimize other performance-critical parts of the simulator, such as memory access patterns or specific complex instruction semantics.
Advanced MLIR Optimizations: Explore more MLIR techniques, such as aggressive dead code elimination, partial evaluation, or specialized data layout transformations based on ISA characteristics.
Leverage JIT Capabilities: Investigate dynamic optimization techniques using [reference missing in original text], such as adaptive compilation based on runtime performance analysis, or generating specialized code for frequently executed basic blocks (trace JIT concepts).
Support More Sail Features: Extend the Jib dialect and lowering passes to fully support the Sail language and its standard library.
Cross-ISA Applicability: Evaluate the applicability of our method to simulators generated from Sail specifications of other ISAs.

= Results and Conslusion <sec:conslusion>

This paper presents a novel MLIR-based compilation backend for Sail, aimed at accelerating the performance of automatically generated ISA simulators. We introduced a new MLIR dialect for Sail's intermediate language, Jib, providing a flexible representation for applying domain-specific compiler optimizations. We demonstrated that optimized IR can be efficiently executed at runtime by implementing specialized transformations targeting arbitrary-precision bit-vector arithmetic and instruction decoding logic, and by integrating the MLIR compilation pipeline with the LLVM JIT framework, represent the potential of leveraging modern compiler infrastructures like MLIR for optimizing DSL-generated code in processor simulation. This work is a foundational contribution towards building high-performance simulators directly from formal ISA specifications, bridging the gap between formal correctness and practical performance. The MLIR-based dialect and optimization framework lay a solid foundation for future research and development in accelerating ISA simulators and other tools derived from Sail and potentially other hardware description DSLs.
